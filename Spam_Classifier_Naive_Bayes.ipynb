{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iynReNRzkix4"
      },
      "source": [
        "# CODE TO DETECT SPAM E-MAILS USING NAIVE BAYES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cirRlXZukiyB"
      },
      "source": [
        "# PROBLEM STATEMENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygxrqHF0kiyC"
      },
      "source": [
        "- The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.\n",
        "\n",
        "- The files contain one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PACLubXUkiyE"
      },
      "source": [
        "# STEP #0: LIBRARIES IMPORT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgOZo0wjkiyF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from google.colab import files #library to upload files to colab notebook\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "r28g1KS8kwHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yFyFgDekiyI"
      },
      "source": [
        "# STEP #1: IMPORT DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cMGqW4VkiyK"
      },
      "outputs": [],
      "source": [
        "spam_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/emails.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAoM0MfCkiyM"
      },
      "outputs": [],
      "source": [
        "spam_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "JZ6CqJ_ikiyN"
      },
      "outputs": [],
      "source": [
        "spam_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHchq0MDkiyO"
      },
      "outputs": [],
      "source": [
        "spam_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "adltD066kiyP"
      },
      "outputs": [],
      "source": [
        "spam_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX2KiYYFkiyQ"
      },
      "source": [
        "# STEP #2: VISUALIZE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MAPERQgkiyR"
      },
      "outputs": [],
      "source": [
        "# Let's see which message is the most popular ham/spam message\n",
        "spam_df.groupby('spam').describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnvS93x3kiyS"
      },
      "outputs": [],
      "source": [
        "# Let's get the length of the messages\n",
        "spam_df['length'] = spam_df['text'].apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFyLcxW8kiyS"
      },
      "outputs": [],
      "source": [
        "spam_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvksbboNkiyT"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x = spam_df['spam'], label = \"Count\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNHATrqmkiyU"
      },
      "source": [
        "# STEP #3: CREATE TESTING AND TRAINING DATASET/DATA CLEANING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AODdMF0-kiyU"
      },
      "source": [
        "# STEP 3.1 EXERCISE: REMOVE PUNCTUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFlFCIJXkiyU"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gec7KM79kiyV"
      },
      "outputs": [],
      "source": [
        "Test = 'Hello Mr. Future, I am so happy to be learning AI now!!'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgQAmDElkiyV"
      },
      "outputs": [],
      "source": [
        "Test_punc_removed = [char for char in Test if char not in string.punctuation]\n",
        "Test_punc_removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCkNAHEKkiyW"
      },
      "outputs": [],
      "source": [
        "# Join the characters again to form the string.\n",
        "Test_punc_removed_join = ''.join(Test_punc_removed)\n",
        "Test_punc_removed_join"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta9DOot0kiyX"
      },
      "source": [
        "# STEP 3.2 EXERCISE: REMOVE STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vY53kewckiyX"
      },
      "outputs": [],
      "source": [
        "# You have to download stopwords Package to execute this command\n",
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQGTeXQUkiyY"
      },
      "outputs": [],
      "source": [
        "Test_punc_removed_join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8Mu9wHJkiyY"
      },
      "outputs": [],
      "source": [
        "Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HZosQKbkiyZ"
      },
      "outputs": [],
      "source": [
        "Test_punc_removed_join_clean # Only important (no so common) words are left"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA_taTMXkiyb"
      },
      "source": [
        "# STEP 3.3 EXERCISE: COUNT VECTORIZER EXAMPLE "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koNMpdjxkiyb"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "sample_data = ['This is the first document.','This document is the second document.','And this is the third one.','Is this the first document?']\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(sample_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICLnu8iVkiyc"
      },
      "outputs": [],
      "source": [
        "print(vectorizer.get_feature_names())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3L7BvOthkiyc"
      },
      "outputs": [],
      "source": [
        "print(X.toarray())  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO-iFLIvkiyd"
      },
      "source": [
        "# LET'S APPLY THE PREVIOUS THREE PROCESSES TO OUR SPAM/HAM EXAMPLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZfaMo7mkiyd"
      },
      "outputs": [],
      "source": [
        "# Let's define a pipeline to clean up all the messages \n",
        "# The pipeline performs the following: (1) remove punctuation, (2) remove stopwords\n",
        "\n",
        "def message_cleaning(message):\n",
        "    Test_punc_removed = [char for char in message if char not in string.punctuation]\n",
        "    Test_punc_removed_join = ''.join(Test_punc_removed)\n",
        "    Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]\n",
        "    return Test_punc_removed_join_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWZ4Ez2Skiye"
      },
      "outputs": [],
      "source": [
        "# Let's test the newly added function\n",
        "spam_df_clean = spam_df['text'].apply(message_cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsYGjW26kiye"
      },
      "outputs": [],
      "source": [
        "print(spam_df_clean[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sy8Uy5okiyf"
      },
      "outputs": [],
      "source": [
        "print(spam_df['text'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-yuBWjFkiyf"
      },
      "source": [
        "# LET'S APPLY COUNT VECTORIZER TO OUR MESSAGES LIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-i4vSpHkiyg"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Define the cleaning pipeline we defined earlier\n",
        "vectorizer = CountVectorizer(analyzer = message_cleaning)\n",
        "spamham_countvectorizer = vectorizer.fit_transform(spam_df['text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k62sO5D_kiyh"
      },
      "outputs": [],
      "source": [
        "print(spamham_countvectorizer.toarray())  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32ab3lQ2kiyi"
      },
      "outputs": [],
      "source": [
        "spamham_countvectorizer.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRi2SqOykiyi"
      },
      "source": [
        "# STEP#4: TRAINING THE MODEL WITH ALL DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEAx4D0lkiyi"
      },
      "outputs": [],
      "source": [
        "NB_classifier = MultinomialNB()\n",
        "label = spam_df['spam'].values\n",
        "NB_classifier.fit(spamham_countvectorizer, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HU2dOwd5kiyi"
      },
      "outputs": [],
      "source": [
        "testing_sample = ['Free money!!!', \"Hi Kim, Please let me know if you need any further information. Thanks\"]\n",
        "testing_sample_countvectorizer = vectorizer.transform(testing_sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ47rfN_kiyj"
      },
      "outputs": [],
      "source": [
        "test_predict = NB_classifier.predict(testing_sample_countvectorizer)\n",
        "test_predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sFWde-akiyk"
      },
      "source": [
        "# STEP#4: DIVIDE THE DATA INTO TRAINING AND TESTING PRIOR TO TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rlyq0XWdkiyk"
      },
      "outputs": [],
      "source": [
        "X = spamham_countvectorizer\n",
        "y = label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TixY2snfkiyk"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vJDsXgNkiyl"
      },
      "outputs": [],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdYDA-u7kiyl"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvozSgunkiym"
      },
      "outputs": [],
      "source": [
        "NB_classifier = MultinomialNB()\n",
        "NB_classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyZngwl8kiyn"
      },
      "source": [
        "# STEP#5: EVALUATING THE MODEL "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUt_2k4ikiyn"
      },
      "outputs": [],
      "source": [
        "y_predict_train = NB_classifier.predict(X_train)\n",
        "y_predict_train\n",
        "cm = confusion_matrix(y_train, y_predict_train)\n",
        "sns.heatmap(cm, annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iSJcttwkiyo"
      },
      "outputs": [],
      "source": [
        "# Predicting the Test set results\n",
        "y_predict_test = NB_classifier.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_predict_test)\n",
        "sns.heatmap(cm, annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEsY36pfkiyo"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_predict_test))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}